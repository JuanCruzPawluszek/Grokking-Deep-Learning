{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Tensor(object):\n",
    "    def __init__(self, data, autograd=False, creators=None, creation_op=None, id=None):\n",
    "        self.data = np.array(data)\n",
    "        self.creation_op = creation_op\n",
    "        self.creators = creators\n",
    "        self.grad = None\n",
    "        self.autograd = autograd\n",
    "        self.children = {}\n",
    "        if (id is None):\n",
    "            id = np.random.randint(0, 100000)\n",
    "        self.id = id\n",
    "        \n",
    "        if (creators is not None):\n",
    "            for c in creators:\n",
    "                if (self.id not in c.children):\n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "    \n",
    "    def all_children_grads_accounted_for(self):\n",
    "        for id,cnt in self.children.items():\n",
    "            if (cnt != 0):\n",
    "                return False\n",
    "        return True\n",
    "        \n",
    "    def backward(self, grad=None, grad_origin=None):\n",
    "        if (self.autograd):\n",
    "            if (grad is None):\n",
    "                grad = Tensor(np.ones_like(self.data))\n",
    "            \n",
    "            if (grad_origin is not None):\n",
    "                if (self.children[grad_origin.id] == 0):\n",
    "                    return\n",
    "                    print(self.id)\n",
    "                    print(self.creation_op)\n",
    "                    print(len(self.creators))\n",
    "                    for c in self.creators:\n",
    "                        print(c.creation_op)\n",
    "                    raise Exception(\"cannot backprop more than once\")\n",
    "                else:\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "            \n",
    "            if (self.grad is None):\n",
    "                self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad\n",
    "                \n",
    "            assert grad.autograd == False\n",
    "       \n",
    "            if (self.creators is not None and (self.all_children_grads_accounted_for() or\n",
    "                                               grad_origin is None)):\n",
    "                if (self.creation_op == \"add\"):\n",
    "                    self.creators[0].backward(grad, self)\n",
    "                    self.creators[1].backward(grad, self)\n",
    "                    \n",
    "                if (self.creation_op == \"neg\"):\n",
    "                    self.creators[0].backward(self.grad.__neg__())\n",
    "                    \n",
    "                if (self.creation_op == \"sub\"):\n",
    "                    new = Tensor(self.grad.data)\n",
    "                    self.creators[0].backward(new, self)\n",
    "                    new = Tensor(self.grad.__neg__().data)\n",
    "                    self.creators[1].backward(new, self)\n",
    "                    \n",
    "                if (self.creation_op == \"mul\"):\n",
    "                    new = self.grad * self.creators[1]\n",
    "                    self.creators[0].backward(new, self)\n",
    "                    new = self.grad * self.creators[0]\n",
    "                    self.creators[1].backward(new, self)\n",
    "                    \n",
    "                if (self.creation_op == \"mm\"):\n",
    "                    act = self.creators[0]\n",
    "                    weights = self.creators[1]\n",
    "                    new = self.grad.mm(weights.transpose())\n",
    "                    act.backward(new)\n",
    "                    new = self.grad.transpose().mm(act).transpose()\n",
    "                    weights.backward(new)\n",
    "                    \n",
    "                if (self.creation_op == \"transpose\"):\n",
    "                    self.creators[0].backward(self.grad.transpose())\n",
    "                    \n",
    "                if (self.creation_op == \"sigmoid\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (self * (ones - self)))\n",
    "                    \n",
    "                if (self.creation_op == \"tanh\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (ones - (self * self)))\n",
    "                    \n",
    "                if (self.creation_op == \"index_select\"):\n",
    "                    new_grad = np.zeros_like(self.creators[0].data)\n",
    "                    indices_ = self.index_select_indices.data.flatten()\n",
    "                    grad_ = grad.data.reshape(len(indices_), -1)\n",
    "                    for i in range(len(indices_)):\n",
    "                        new_grad[indices_[i]] += grad_[i]\n",
    "                    self.creators[0].backward(Tensor(new_grad))\n",
    "                    \n",
    "                if (self.creation_op == \"cross_entropy\"):\n",
    "                    dx = self.softmax_output - self.target_dist\n",
    "                    self.creators[0].backward(Tensor(dx))\n",
    "                    \n",
    "                if (\"sum\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    ds = self.creators[0].data.shape[dim]\n",
    "                    self.creators[0].backward(self.grad.expand(dim, ds))\n",
    "                    \n",
    "                if (\"expand\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.sum(dim))\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        if (self.autograd and other.autograd):\n",
    "            return Tensor(self.data + other.data, autograd=True, creators=[self,other],\n",
    "                          creation_op=\"add\")\n",
    "        return Tensor(self.data + other.data)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())\n",
    "    \n",
    "    def __neg__(self):\n",
    "        if (self.autograd):\n",
    "            return Tensor(self.data * -1, autograd=True, creators=[self], creation_op=\"neg\")\n",
    "        return Tensor(self.data * -1)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if (self.autograd and other.autograd):\n",
    "            return Tensor(self.data - other.data, autograd=True, creators=[self,other], \n",
    "                          creation_op=\"sub\")\n",
    "        return Tensor(self.data - other.data)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        if (self.autograd and other.autograd):\n",
    "            return Tensor(self.data * other.data, autograd=True, creators=[self,other],\n",
    "                          creation_op=\"mul\")\n",
    "        return Tensor(self.data * other.data)\n",
    "    \n",
    "    def sum(self, dim):\n",
    "        if (self.autograd):\n",
    "            return Tensor(self.data.sum(dim), autograd=True, creators=[self],\n",
    "                          creation_op=\"sum_\"+str(dim))\n",
    "        return Tensor(self.data.sum(dim))\n",
    "    \n",
    "    def expand(self, dim, copies):\n",
    "        trans_cmd = list(range(0, len(self.data.shape)))\n",
    "        trans_cmd.insert(dim, len(self.data.shape))\n",
    "        new_shape = list(self.data.shape) + [copies]\n",
    "        new_data = self.data.repeat(copies).reshape(new_shape)\n",
    "        new_data = new_data.transpose(trans_cmd)\n",
    "        \n",
    "        if (self.autograd):\n",
    "            return Tensor(new_data, autograd=True, creators=[self], creation_op=\"expand_\"+str(dim))\n",
    "        return Tensor(new_data)\n",
    "    \n",
    "    def transpose(self):\n",
    "        if (self.autograd):\n",
    "            return Tensor(self.data.transpose(), autograd=True, creators=[self],\n",
    "                          creation_op=\"transpose\")\n",
    "        return Tensor(self.data.transpose())\n",
    "    \n",
    "    def mm(self, x):\n",
    "        if (self.autograd):\n",
    "            return Tensor(self.data.dot(x.data), autograd=True, creators=[self,x], creation_op=\"mm\")\n",
    "        return Tensor(self.data.dot(x.data))\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        if (self.autograd):\n",
    "            return Tensor(1 / (1 + np.exp(-self.data)), autograd=True, creators=[self],\n",
    "                          creation_op=\"sigmoid\")\n",
    "        return Tensor(1 / (1 + np.exp(-self.data)))\n",
    "    \n",
    "    def tanh(self):\n",
    "        if (self.autograd):\n",
    "            return Tensor(np.tanh(self.data), autograd=True, creators=[self], creation_op=\"tanh\")\n",
    "        return Tensor(np.tanh(self.data))\n",
    "    \n",
    "    ############################################################################\n",
    "    def softmax(self):\n",
    "        temp = np.exp(self.data)\n",
    "        softmax_output = temp / np.sum(temp, axis=len(self.data.shape)-1, keepdims=True)\n",
    "        return softmax_output\n",
    "    ############################################################################\n",
    "    \n",
    "    def index_select(self, indices):\n",
    "        if (self.autograd):\n",
    "            new = Tensor(self.data[indices.data], autograd=True, creators=[self], \n",
    "                         creation_op=\"index_select\")\n",
    "            new.index_select_indices = indices\n",
    "            return new\n",
    "        return Tensor(self.data[indices.data])\n",
    "    \n",
    "    def cross_entropy(self, target_indices):\n",
    "        temp = np.exp(self.data)\n",
    "        softmax_output = temp / np.sum(temp, axis=len(self.data.shape)-1, keepdims=True)\n",
    "        \n",
    "        t = target_indices.data.flatten()\n",
    "        p = softmax_output.reshape(len(t), -1)\n",
    "        target_dist = np.eye(p.shape[1])[t]\n",
    "        loss = -(np.log(p) * (target_dist)).sum(1).mean()\n",
    "        \n",
    "        if (self.autograd):\n",
    "            out = Tensor(loss, autograd=True, creators=[self], creation_op=\"cross_entropy\")\n",
    "            out.softmax_output = softmax_output\n",
    "            out.target_dist = target_dist\n",
    "            return out\n",
    "        \n",
    "        return Tensor(loss)\n",
    "    \n",
    "    \n",
    "class SGD(object):\n",
    "    \n",
    "    def __init__(self, parameters, alpha=0.1):\n",
    "        self.parameters = parameters\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def zero(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad.data *= 0\n",
    "            \n",
    "    def step(self, zero=True):\n",
    "        for p in self.parameters:\n",
    "            p.data -= p.grad.data * self.alpha\n",
    "            \n",
    "            if (zero):\n",
    "                p.grad.data *= 0\n",
    "           \n",
    "        \n",
    "class Layer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.parameters = list()\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        return self.parameters\n",
    "    \n",
    "    \n",
    "class Linear(Layer):\n",
    "    \n",
    "    def __init__(self, n_inputs, n_outputs, bias=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.use_bias = bias\n",
    "        \n",
    "        W = np.random.randn(n_inputs, n_outputs)*np.sqrt(2.0/(n_inputs))\n",
    "        self.weight = Tensor(W, autograd=True)\n",
    "        if (self.use_bias):\n",
    "            self.bias = Tensor(np.zeros(n_outputs), autograd=True)\n",
    "        \n",
    "        self.parameters.append(self.weight)\n",
    "        \n",
    "        if (self.use_bias):\n",
    "            self.parameters.append(self.bias)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        if (self.use_bias):\n",
    "            return inp.mm(self.weight) + self.bias.expand(0, len(inp.data))\n",
    "        return inp.mm(self.weight)\n",
    "    \n",
    "    \n",
    "class Sequential(Layer):\n",
    "    def __init__(self, layers=list()):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = layers\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        for layer in self.layers:\n",
    "            inp = layer.forward(inp)\n",
    "        return inp\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        params = list()\n",
    "        for l in self.layers:\n",
    "            params += l.get_parameters()\n",
    "        return params\n",
    "    \n",
    "    \n",
    "class MSELoss(Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        return ((pred - target) * (pred - target)).sum(0)\n",
    "    \n",
    "    \n",
    "class Tanh(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        return inp.tanh()\n",
    "    \n",
    "    \n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        return inp.sigmoid()\n",
    "\n",
    "    \n",
    "class Embedding(Layer):\n",
    "    def __init__(self, vocab_size, dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim = dim\n",
    "        \n",
    "        weight = (np.random.rand(vocab_size, dim) - 0.5) / dim\n",
    "        \n",
    "        # with index_select method created we can forward prop\n",
    "        self.weight = Tensor(weight, autograd=True)\n",
    "        self.parameters.append(self.weight)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        return self.weight.index_select(inp)\n",
    "    \n",
    "    \n",
    "class CrossEntropyLoss(object):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, inp, target):\n",
    "        return inp.cross_entropy(target)\n",
    "    \n",
    "    \n",
    "class RNNCell(Layer):\n",
    "    def __init__(self, n_inputs, n_hidden, n_output, activation='sigmoid'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        \n",
    "        if (activation == 'sigmoid'):\n",
    "            self.activation = Sigmoid()\n",
    "        elif (activation == 'tanh'):\n",
    "            self.activation = Tanh()\n",
    "        else:\n",
    "            raise Exception(\"Non-linearity not found\")\n",
    "            \n",
    "        self.w_ih = Linear(n_inputs, n_hidden)\n",
    "        self.w_hh = Linear(n_hidden, n_hidden)\n",
    "        self.w_ho = Linear(n_hidden, n_output)\n",
    "        \n",
    "        self.parameters += self.w_ih.get_parameters()\n",
    "        self.parameters += self.w_hh.get_parameters()\n",
    "        self.parameters += self.w_ho.get_parameters()\n",
    "        \n",
    "    def forward(self, inp, hidden):\n",
    "        from_prev_hidden = self.w_hh.forward(hidden)\n",
    "        combined = self.w_ih.forward(inp) + from_prev_hidden\n",
    "        new_hidden = self.activation.forward(combined)\n",
    "        output = self.w_ho.forward(new_hidden)\n",
    "        return output, new_hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return Tensor(np.zeros((batch_size, self.n_hidden)), autograd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, random, math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "f = open('shakespeare.txt', 'r')\n",
    "raw = f.read()\n",
    "f.close()\n",
    "\n",
    "vocab = list(set(raw))\n",
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "indices = np.array(list(map(lambda x:word2index[x], raw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedding(vocab_size=len(vocab), dim=512)\n",
    "model = RNNCell(n_inputs=512, n_hidden=512, n_output=len(vocab))\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "optim = SGD(parameters=model.get_parameters() + embed.get_parameters(), alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "bptt = 16\n",
    "n_batches = int((indices.shape[0] / (batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_indices = indices[:n_batches*batch_size]\n",
    "batched_indices = trimmed_indices.reshape(batch_size, n_batches)\n",
    "batched_indices = batched_indices.transpose()\n",
    "\n",
    "input_batched_indices = batched_indices[0:-1]\n",
    "target_batched_indices = batched_indices[1:]\n",
    "\n",
    "n_bptt = int(((n_batches-1) / bptt))\n",
    "input_batches = input_batched_indices[:n_bptt*bptt]\n",
    "input_batches = input_batches.reshape(n_bptt, bptt, batch_size)\n",
    "target_batches = target_batched_indices[:n_bptt*bptt]\n",
    "target_batches = target_batches.reshape(n_bptt, bptt, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First\n",
      "[29 10 50 23 60]\n"
     ]
    }
   ],
   "source": [
    "print(raw[0:5])\n",
    "print(indices[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[29 63 30  5 48 35 30  3 30 30 23 38 16 27  3  3  8  8  3  8 58 16 16 30\n",
      "  16  8  8 61 52  8 19 58]\n",
      " [10  3 32  3  0  2  7 52  5 63  8  3  0 24  7 37 30 59 25 25 60 16 23  7\n",
      "  16 59 18 50 53  8 30 53]\n",
      " [50 50  3 23 30 46 58 30  3  3 58 50 27  5 14 48 63  0  0  8 30 30 30  3\n",
      "  30 27 30  3  5 30 63  8]\n",
      " [23 30 50  8  3 13 25 38 38  3  0 23 31 58 10 41 16 30 30  7 10 37 14 50\n",
      "   3 61 10 39 30 14 16 50]\n",
      " [60  2 10 30 50 17 30  8 30 60 27  8  3 60 25 30 58 63 63 19 23  3  8  8\n",
      "  25  8 63  3 19  8 58  0]]\n"
     ]
    }
   ],
   "source": [
    "print(batched_indices[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(iterations=100):\n",
    "    for iter in range(iterations):\n",
    "        total_loss = 0\n",
    "        n_loss = 0\n",
    "        \n",
    "        hidden = model.init_hidden(batch_size=batch_size)\n",
    "        for batch_i in range(len(input_batches)):\n",
    "            \n",
    "            hidden = Tensor(hidden.data, autograd=True)\n",
    "            loss = None\n",
    "            losses = list()\n",
    "            for t in range(bptt):\n",
    "                inp = Tensor(input_batches[batch_i][t], autograd=True)\n",
    "                rnn_input = embed.forward(inp=inp)\n",
    "                output, hidden = model.forward(inp=rnn_input, hidden=hidden)\n",
    "                \n",
    "                target = Tensor(target_batches[batch_i][t], autograd=True)\n",
    "                batch_loss = criterion.forward(output, target)\n",
    "                losses.append(batch_loss)\n",
    "                if (t == 0):\n",
    "                    loss = batch_loss\n",
    "                else:\n",
    "                    loss = loss + batch_loss\n",
    "            for loss in losses:\n",
    "                \"\"\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            total_loss += loss.data\n",
    "            log = \"\\r Iter: \" + str(iter)\n",
    "            log += \" - Batch \" + str(batch_i+1) + \"/\" + str(len(input_batches))\n",
    "            log += \" - Loss: \" + str(np.exp(total_loss / (batch_i+1)))\n",
    "            if (batch_i == 0):\n",
    "                log += \" - \" + generate_sample(70, '\\n'.replace(\"\\n\", \" \"))\n",
    "            if (batch_i % 10 == 0 or batch_i-1 == len(input_batches)):\n",
    "                sys.stdout.write(log)\n",
    "        optim.alpha *= 0.99\n",
    "        print()\n",
    "        \n",
    "def generate_sample(n=30, init_char=' '):\n",
    "    s = \"\"\n",
    "    hidden = model.init_hidden(batch_size=1)\n",
    "    inp = Tensor(np.array([word2index[init_char]]))\n",
    "    for i in range(n):\n",
    "        rnn_input = embed.forward(inp)\n",
    "        output, hidden = model.forward(inp=rnn_input, hidden=hidden)\n",
    "        output.data *= 10\n",
    "        temp_dist = output.softmax()\n",
    "        temp_dist /= temp_dist.sum()\n",
    "        \n",
    "        m = (temp_dist > np.random.rand()).argmax()\n",
    "        c = vocab[m]\n",
    "        inp = Tensor(np.array([m]))\n",
    "        s += c\n",
    "        \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter: 0 - Batch 2171/2178 - Loss: 13.659292582387012 ,   ,              ,            ,    ,               ,  ,           \n",
      " Iter: 1 - Batch 2171/2178 - Loss: 8.849821110311202 ther and and and and and and and and and and and and and and and and a\n",
      " Iter: 2 - Batch 2171/2178 - Loss: 7.8622356204813715he so s,er and and and and and and and and and and and ,o h, and and \n",
      " Iter: 3 - Batch 2171/2178 - Loss: 7.2725368280760465, word and and and and and and ,o ,o ,o steave and ,o ,o ,o ,o stea, \n",
      " Iter: 4 - Batch 2171/2178 - Loss: 6.8490762174687035he somes and ,o ,o ,o s,eath the ,o so so ,o son ,o ,o s,eath the son\n",
      " Iter: 5 - Batch 2171/2178 - Loss: 6.5129394450884655onges and and ,o have and the so, and ,o st, and and the so, and the \n",
      " Iter: 6 - Batch 2171/2178 - Loss: 6.2335223021547965nd and the souse ,o bear and so the so, and the so, and so ,o so the\n",
      " Iter: 7 - Batch 2171/2178 - Loss: 5.9953072949702145, and the souse ,o s,, and the souse ,o stear and the souse and the s\n",
      " Iter: 8 - Batch 2171/2178 - Loss: 5.7887313754350355ongess and s,own ,o so the so the souse ,o s,own ,o s,own so the so, \n",
      " Iter: 9 - Batch 2171/2178 - Loss: 5.6085912349697536ere and the soun, and the souse the sou, and the sound and so ,o stea\n",
      " Iter: 10 - Batch 2171/2178 - Loss: 5.4491991668352456, and so and the s,ong and ,o ,o ,o steart, and ,o steak of the soun\n",
      " Iter: 11 - Batch 2171/2178 - Loss: 5.3053123365458356ee the s, and sound, and ,o ,o bear the sound, and ,o ,o bear and ,o \n",
      " Iter: 12 - Batch 2171/2178 - Loss: 5.1739573021163935, and so and soundress and soundress and so and so, and so, and s, an\n",
      " Iter: 13 - Batch 1/2178 - Loss: 5.363622697474662 - s, and s, and so and so, and s, and so his ,\n",
      "The sta,\n",
      " Iter: 13 - Batch 2171/2178 - Loss: 5.0520408368243025\n",
      " Iter: 14 - Batch 1/2178 - Loss: 5.1926279637383645 - s, and so and ,o ,o her have and so, and so, and so and ,o her ,\n",
      " Iter: 14 - Batch 2171/2178 - Loss: 4.9356834317819715\n",
      " Iter: 15 - Batch 1/2178 - Loss: 5.047177418723767 - are ,o be stea, and ,o h, and ,o ,o h, and so ,o ,o her ,\n",
      " Iter: 15 - Batch 2171/2178 - Loss: 4.8256918259214815\n",
      " Iter: 16 - Batch 2171/2178 - Loss: 4.7227012840604594re and so his s, and so and ,o ,o her and so and so and ,o ,o her ,, \n",
      " Iter: 17 - Batch 2171/2178 - Loss: 4.6233828170021795e and ,o ,own and so ,our g, and ,o ,o, and so stear and ,o ,o, and \n",
      " Iter: 18 - Batch 1/2178 - Loss: 4.842701348587198 - arrang,\n",
      " Iter: 18 - Batch 2171/2178 - Loss: 4.5286608065987055ur sound\n",
      " Iter: 19 - Batch 2171/2178 - Loss: 4.4355382187059895r, and so stear and so stear , and so stear and so stear and so ,, go\n",
      " Iter: 20 - Batch 2171/2178 - Loss: 4.3447362678918785re and ,o bear his , and so , and so , and ,, and so , and , and so s\n",
      " Iter: 21 - Batch 1/2178 - Loss: 4.523637365965966 - are and be stear and ,\n",
      " Iter: 21 - Batch 2171/2178 - Loss: 4.2572372235828855\n",
      " Iter: 22 - Batch 2171/2178 - Loss: 4.1728479249935575re and , and so steak the , and , and so , and so , and the words and\n",
      " Iter: 23 - Batch 2171/2178 - Loss: 4.0949290712547986re and , and , and , and , and so , and be steak the words and s, and\n",
      " Iter: 24 - Batch 1/2178 - Loss: 4.587255584266162 - are and , and so and the words and a,\n",
      " Iter: 24 - Batch 2171/2178 - Loss: 4.0100074665103625\n",
      " Iter: 25 - Batch 2171/2178 - Loss: 3.9343661777852742re and so and , and , and , and so and , and , and , and , and , and \n",
      " Iter: 26 - Batch 1/2178 - Loss: 4.47434232848499 - are and so forged ,, and all the words,\n",
      " Iter: 26 - Batch 2171/2178 - Loss: 3.8531401100124176\n",
      " Iter: 27 - Batch 1/2178 - Loss: 4.635052736073254 - arthis grave the words,\n",
      "Assir, and all the words,\n",
      " Iter: 27 - Batch 2171/2178 - Loss: 3.7904307788485245\n",
      " Iter: 28 - Batch 1/2178 - Loss: 4.422307755086847 - ar, and the , and the words, and all the words, and a,\n",
      " Iter: 28 - Batch 2171/2178 - Loss: 3.7329244792481937\n",
      " Iter: 29 - Batch 1/2178 - Loss: 4.927364027311746 - ar, and the words,, and , good and the words,\n",
      "Assentle,\n",
      " Iter: 29 - Batch 2171/2178 - Loss: 3.6657483245497215\n",
      " Iter: 30 - Batch 2171/2178 - Loss: 3.6038425800697853rrow, and all the words and the words and the words and the words and\n",
      " Iter: 31 - Batch 2171/2178 - Loss: 3.5590114062125053he , and the words and all the , and the words and the words and the \n",
      " Iter: 32 - Batch 1/2178 - Loss: 5.244931731298869 - great the words and the words and the words and the words,\n",
      " Iter: 32 - Batch 2171/2178 - Loss: 3.4979579675508533\n",
      " Iter: 33 - Batch 2171/2178 - Loss: 3.4422018116377204he words and the w, good , give me steak the words and the words and \n",
      " Iter: 34 - Batch 1/2178 - Loss: 5.268238004982989 - greatest.\n",
      "\n",
      "ROMEO:\n",
      "Sir, and the wick, ,wear ,\n",
      " Iter: 34 - Batch 2171/2178 - Loss: 3.4053437604206134\n",
      " Iter: 35 - Batch 1/2178 - Loss: 5.265223458629692 - great the words and the words and the words and ,\n",
      "Whe,\n",
      "A,teaves\n",
      " Iter: 35 - Batch 2171/2178 - Loss: 3.3357521749456096\n",
      " Iter: 36 - Batch 1/2178 - Loss: 4.886906666624087 - great the words and the words and ,were in this are a,\n",
      " Iter: 36 - Batch 2171/2178 - Loss: 3.2869051794757898\n",
      " Iter: 37 - Batch 2171/2178 - Loss: 3.2455333941317845reat and the , and the words and the , and ,o have a, and save the go\n",
      " Iter: 38 - Batch 1/2178 - Loss: 6.2247810181423056 - great ,\n",
      " Iter: 38 - Batch 2171/2178 - Loss: 3.1973032504539153dly and \n",
      " Iter: 39 - Batch 2171/2178 - Loss: 3.1894115168516097great h, and the grave the grave the grave the grave the grave the gra\n",
      " Iter: 40 - Batch 1/2178 - Loss: 4.936367750478989 - great part ,u, and all the words,\n",
      "Alive a man.\n",
      "\n",
      "ROMEO:\n",
      "Aland's face ,\n",
      " Iter: 40 - Batch 2171/2178 - Loss: 3.1585186806448234\n",
      " Iter: 41 - Batch 1/2178 - Loss: 4.91583259261229 - great great her ,\n",
      "And save an a,\n",
      " Iter: 41 - Batch 2171/2178 - Loss: 3.0996606370600475\n",
      " Iter: 42 - Batch 2171/2178 - Loss: 3.0771924039547098eat and wordling and wordling and wordling and wordling and wordling\n",
      " Iter: 43 - Batch 1/2178 - Loss: 4.7794384304619975 - great ,\n",
      " Iter: 43 - Batch 2171/2178 - Loss: 3.0169677038417433he wing,\n",
      " Iter: 44 - Batch 2171/2178 - Loss: 3.0071780425761205great and the words,, a,vifience, and the words,, a,vifience, and the \n",
      " Iter: 45 - Batch 2171/2178 - Loss: 2.9631437441043666rich her steed and wordly, then ,o be her steed, then the word and wo\n",
      " Iter: 46 - Batch 1/2178 - Loss: 4.212263266861946 - great us , good restle.\n",
      "\n",
      " Iter: 46 - Batch 2171/2178 - Loss: 2.9705606830650813\n",
      " Iter: 47 - Batch 2171/2178 - Loss: 2.9306627821871316eat his , good forgoress, and the ,ight and the ,ight and the goors.\n",
      " Iter: 48 - Batch 1/2178 - Loss: 4.086171971398275 - are the gold me.\n",
      "\n",
      "SAMP,o, the gold me.\n",
      "\n",
      "SAMPETHEN ESWARWICK:\n",
      " Iter: 48 - Batch 2171/2178 - Loss: 2.9330509152508615\n",
      " Iter: 49 - Batch 1/2178 - Loss: 4.208622044330869 - answer hath service is a grave the ,utury,\n",
      " Iter: 49 - Batch 2171/2178 - Loss: 2.9342473835231697\n",
      " Iter: 50 - Batch 1/2178 - Loss: 3.9828220147329763 - amples ,\n",
      "And ,o be ,\n",
      " Iter: 50 - Batch 2171/2178 - Loss: 2.8589280726586934\n",
      " Iter: 51 - Batch 2171/2178 - Loss: 2.8199059005255487answer him on the wing and saver and words, and the name; and the name\n",
      " Iter: 52 - Batch 2171/2178 - Loss: 2.7914490293278875 see his forbing him we ,i, and words and ,ute is endreatester upon a\n",
      " Iter: 53 - Batch 1/2178 - Loss: 5.015726580267204 - and saver are all there,\n",
      " Iter: 53 - Batch 2171/2178 - Loss: 2.7617654314212545\n",
      " Iter: 54 - Batch 2171/2178 - Loss: 2.7898497074297655answer hath prook'st my bro, h, agood for, and say it a gale stean the\n",
      " Iter: 55 - Batch 1/2178 - Loss: 3.376537418417945 - a, good Rome an ause.\n",
      "\n",
      "ROMEO:\n",
      " Iter: 55 - Batch 2171/2178 - Loss: 2.7395301055447003\n",
      " Iter: 56 - Batch 1/2178 - Loss: 3.4351111789680284 - a s, wherive the sup the ,uries?\n",
      "\n",
      "MENENIUS:\n",
      " Iter: 56 - Batch 2171/2178 - Loss: 2.7069626430632363\n",
      " Iter: 57 - Batch 2171/2178 - Loss: 2.6806967790638815n all the plofal to ,ute you, Heave to heart steeling and worshape, a\n",
      " Iter: 58 - Batch 2171/2178 - Loss: 2.7070238086659093answering and worl word and worl word and worl word and worl word and \n",
      " Iter: 59 - Batch 1/2178 - Loss: 3.0653101197979984 - a see hi, for a plover: ,'St upon thy amb,\n",
      " Iter: 59 - Batch 2171/2178 - Loss: 2.6996375124594377\n",
      " Iter: 60 - Batch 1/2178 - Loss: 3.321389396477327 - and ,o an old man:\n",
      "And ,o tell the sheerish.\n",
      "\n",
      "ROMEO:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter: 60 - Batch 2171/2178 - Loss: 2.6511611700861915\n",
      " Iter: 61 - Batch 2171/2178 - Loss: 2.6268951387893665nswer his face is the worst cousing ,ing h, were it ,ishous far one i\n",
      " Iter: 62 - Batch 2171/2178 - Loss: 2.5955159215078845a souls ,o the wor, and then, not an old be conscor, and then, not be \n",
      " Iter: 63 - Batch 2171/2178 - Loss: 2.5543273260791914ou best us the words, we have ep,o have a mean thou , all the sup ,o \n",
      " Iter: 64 - Batch 1/2178 - Loss: 2.768538778381009 - a,\n",
      "As I see his hangman:\n",
      " Iter: 64 - Batch 2171/2178 - Loss: 2.5455763033154697\n",
      " Iter: 65 - Batch 2171/2178 - Loss: 2.5400721002849923nswer, and all the sup the s, what I would ,ill's enterselves and wor\n",
      " Iter: 66 - Batch 2171/2178 - Loss: 2.5440454844255095an a ,ime, and whe, and whe, and w, and won awfixchise and wonder the \n",
      " Iter: 67 - Batch 2171/2178 - Loss: 2.4928955805864996re bus not be rest me and words again. I know not be rest me and word\n",
      " Iter: 68 - Batch 2171/2178 - Loss: 2.5108663151053873 see his for a maides have I crees and wh, not a brafe, as I am nothi\n",
      " Iter: 69 - Batch 1/2178 - Loss: 3.8869566127440147 - a sise him ,o aboute, and the, and the name\n",
      "The senses, and my lord.\n",
      "\n",
      " Iter: 69 - Batch 2171/2178 - Loss: 2.5191243763634467\n",
      " Iter: 70 - Batch 2171/2178 - Loss: 2.4838707650341156re an old sent him congeal'st my fing st,, as I am ,o attention i, th\n",
      " Iter: 71 - Batch 1/2178 - Loss: 4.3095281784008 - a see, and my wintty enters!\n",
      " Iter: 71 - Batch 2171/2178 - Loss: 2.4603861912504456\n",
      " Iter: 72 - Batch 2171/2178 - Loss: 2.4390534054941346answer him co,'s death of the name; what a man thou art: the ,, and th\n",
      " Iter: 73 - Batch 1/2178 - Loss: 3.2683569420271903 - a course, or forbif; and ,o artaintag\n",
      " Iter: 73 - Batch 2171/2178 - Loss: 2.4162081520340988\n",
      " Iter: 74 - Batch 1/2178 - Loss: 4.060958627653624 - a sile in perciying forgot use\n",
      " Iter: 74 - Batch 2171/2178 - Loss: 2.4076925504952748\n",
      " Iter: 75 - Batch 1/2178 - Loss: 3.905862556746214 - able despair. Fixt agood and what a mist,\n",
      " Iter: 75 - Batch 2171/2178 - Loss: 2.4227089089507445\n",
      " Iter: 76 - Batch 1/2178 - Loss: 4.4748202922121445 - answer his for a pl,asis for the name;\n",
      "And words were steiness.\n",
      "\n",
      " Iter: 76 - Batch 2171/2178 - Loss: 2.3888918759318916\n",
      " Iter: 77 - Batch 2171/2178 - Loss: 2.3361687859975096 souls say is a mark'd the worshis grace of it is nothing entr, not f\n",
      " Iter: 78 - Batch 1/2178 - Loss: 2.8862767262012823 - a sith me sir. I man,-strange:\n",
      "And say is a glord, ther and worang,\n",
      " Iter: 78 - Batch 2171/2178 - Loss: 2.3606435262138783\n",
      " Iter: 79 - Batch 1/2178 - Loss: 3.2892849921879717 - answerian gre not a love.\n",
      " Iter: 79 - Batch 2171/2178 - Loss: 2.3469157052415363\n",
      " Iter: 80 - Batch 1/2178 - Loss: 3.2633978290167183 - answer of this yours.\n",
      "\n",
      "TRANIO:\n",
      " Iter: 80 - Batch 2171/2178 - Loss: 2.3387645855073664\n",
      " Iter: 81 - Batch 2171/2178 - Loss: 2.2960323490240446 st, we man hame , arm is s, to setters and hus do you are fent; and \n",
      " Iter: 82 - Batch 1/2178 - Loss: 3.233167326975092 - a mus the worstance, and therefore.\n",
      "\n",
      "ROMEO:\n",
      " Iter: 82 - Batch 2171/2178 - Loss: 2.3003986177499473\n",
      " Iter: 83 - Batch 1/2178 - Loss: 3.445024885041201 - a,\n",
      "Go, gentle in to her,\n",
      " Iter: 83 - Batch 2171/2178 - Loss: 2.2922549860826793\n",
      " Iter: 84 - Batch 1/2178 - Loss: 2.5645430214051443 - answerer, all the\n",
      " Iter: 84 - Batch 2171/2178 - Loss: 2.2775604480563763\n",
      " Iter: 85 - Batch 2171/2178 - Loss: 2.2616705948459255nd husbe shall be did in the worst; and my sovereadn: and here, how i\n",
      " Iter: 86 - Batch 2171/2178 - Loss: 2.2232591232726534bout, bring the house of it is a could me rest, and his forture, ,o h\n",
      " Iter: 87 - Batch 1/2178 - Loss: 2.37493672879601 - am, and my ,your ,\n",
      " Iter: 87 - Batch 2171/2178 - Loss: 2.2285703387628693\n",
      " Iter: 88 - Batch 1/2178 - Loss: 2.777502057892701 - answer and even steedy is , are thou love heavy presentag\n",
      " Iter: 88 - Batch 2171/2178 - Loss: 2.2292637167305736\n",
      " Iter: 89 - Batch 2171/2178 - Loss: 2.2328656471851653and ,, is is is is is is is is is is is is is is is is is is is is is \n",
      " Iter: 90 - Batch 2171/2178 - Loss: 2.1766905491125343a sitter and alr the name; how a soul in , all the name; how a soul in\n",
      " Iter: 91 - Batch 2171/2178 - Loss: 2.1909699722583397nswer in the name in the name in the name in the name in the name in \n",
      " Iter: 92 - Batch 2171/2178 - Loss: 2.1509165217711157nd unthen, and world, her children take is seat the shee her love sha\n",
      " Iter: 93 - Batch 1/2178 - Loss: 2.60462021118615 - a see the hous Lartise, and the words, there's\n",
      "Of Petrue,\n",
      " Iter: 93 - Batch 2171/2178 - Loss: 2.1652609916401832\n",
      " Iter: 94 - Batch 2171/2178 - Loss: 2.1804718219606934a breater, ,old them for a man the sider and his , good dear, they a, \n",
      " Iter: 95 - Batch 1/2178 - Loss: 2.4434788802183114 - a sire of cou, and longer, and my lord.\n",
      "\n",
      "KING RICHARD III:\n",
      " Iter: 95 - Batch 2171/2178 - Loss: 2.1521639674560556\n",
      " Iter: 96 - Batch 1/2178 - Loss: 3.270223810150943 - a,\n",
      "Gr, good my ,\n",
      " Iter: 96 - Batch 2171/2178 - Loss: 2.1616543041369635\n",
      " Iter: 97 - Batch 1/2178 - Loss: 2.1999899034093593 - a s, to smy is a count him one hath been thy mossenger:\n",
      " Iter: 97 - Batch 2171/2178 - Loss: 2.1502323959439797\n",
      " Iter: 98 - Batch 1/2178 - Loss: 3.307469617833343 - a sith even speak shore;\n",
      " Iter: 98 - Batch 2171/2178 - Loss: 2.1499265700768513\n",
      " Iter: 99 - Batch 1/2178 - Loss: 2.570025880067571 - a seess with , ambboy!\n",
      "\n",
      "MENENIUS:\n",
      " Iter: 99 - Batch 2171/2178 - Loss: 2.1953107473865463\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer and love shall p,\n",
      "As I bes, thou hast unto himse would be courter o, the shep,\n",
      "As I sent her and world,\n",
      "I wo, the such she sent from and world,\n",
      "I won, besul and I comes before but not ,well have , and ,, a, send t, there is the name is sements and my s, and my wi, her\n",
      "being sir, work in the worst, how in the worst, how in the worst, how in the bed,\n",
      "And I find are , as I su, the such she sent her and ,u, her\n",
      "bentreat you ,\n",
      "And there ,'st my grace.\n",
      "\n",
      "ROMEO:\n",
      "And his grucking bound ,'bould be courter of my see his , besued' the sheptern thou hast unto him w, and regens, and my wife, hereafer the such she sent from and world,\n",
      "I won, besul and I comes before but not ,well hope hath past, on the name is , ,'Tis such , be, and my , are I love shall part with the ,using of , as I sent thou not\n",
      "felth!\n",
      "\n",
      "VOLUMNIAN:\n",
      "Uldespy in the grance. What a death,\n",
      "In all, awake! her\n",
      "bring should be ,iture, the such she sent ,' is grace of could be courter o, the,, and my will be debent h, who do t, and ,'tvin to sent courteen the,\n",
      "In , gentle Rumen of the but not have a man the sheptern thy lord.\n",
      "\n",
      "Second Musiness.\n",
      "\n",
      "Second Musiness.\n",
      "\n",
      "Second Musiness.\n",
      "\n",
      "Second Muncles to sent ,'sif to sent her and world,\n",
      "I wonself the such me restroy slort,\n",
      "Think the such me restroy slorting him one hour one hereates botter, all good s, and there is the name is , besued' the must m, he shall be ,o do you see in the worst, how in the worst, how in the worst, how in the worst, how in the bed,\n",
      "And I find are not this sendense; and my wife, herea, it ,well have met not best and world,\n",
      "I wonse,\n",
      "Rome and ,ubune.\n",
      "\n",
      "Second Munclest unto himse would be courter o, the such she sent ,' is grace of me and love shall p,\n",
      "As I beseech you, ,'t:\n",
      "Cores\n",
      "The such she sent her and hereay is the worst, h, I bear ,\n",
      "And wonto me,\n",
      "And wonto me restrook is the worst, h, I bear her and world,\n",
      "I wonself this is i, were it is the name is the worst, how in the , botter Flarer, peach: you see i, when h, bear her and world,\n",
      "I wonself t\n"
     ]
    }
   ],
   "source": [
    "print(generate_sample(n=2000, init_char='\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small backpropagation with RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid Activations\n",
      "[1. 1.]\n",
      "[1.00678365 1.00678365]\n",
      "[1.00655594 1.00655594]\n",
      "[1.00656346 1.00656346]\n",
      "[1.00656321 1.00656321]\n",
      "[1.00656322 1.00656322]\n",
      "[1.00656322 1.00656322]\n",
      "[1.00656322 1.00656322]\n",
      "[1.00656322 1.00656322]\n",
      "[1.00656322 1.00656322]\n",
      "\n",
      "Sigmoid Gradient\n",
      "[-0.03303147 -0.03303147]\n",
      "[0.00109108 0.00109108]\n",
      "[-3.60399005e-05 -3.60399005e-05]\n",
      "[1.19045078e-06 1.19045078e-06]\n",
      "[-3.93223372e-08 -3.93223372e-08]\n",
      "[1.2988729e-09 1.2988729e-09]\n",
      "[-4.29052588e-11 -4.29052588e-11]\n",
      "[1.41564198e-12 1.41564198e-12]\n",
      "[-4.83418583e-14 -4.83418583e-14]\n",
      "[0. 0.]\n",
      "Activations\n",
      "[5. 5.]\n",
      "[25. 25.]\n",
      "[125. 125.]\n",
      "[625. 625.]\n",
      "[3125. 3125.]\n",
      "[15625. 15625.]\n",
      "[78125. 78125.]\n",
      "[390625. 390625.]\n",
      "[1953125. 1953125.]\n",
      "[9765625. 9765625.]\n",
      "\n",
      "Gradients\n",
      "[5. 5.]\n",
      "[25. 25.]\n",
      "[125. 125.]\n",
      "[625. 625.]\n",
      "[3125. 3125.]\n",
      "[15625. 15625.]\n",
      "[78125. 78125.]\n",
      "[390625. 390625.]\n",
      "[1953125. 1953125.]\n",
      "[9765625. 9765625.]\n"
     ]
    }
   ],
   "source": [
    "(sigmoid, relu) = (lambda x:1/(1-np.exp(-x)), lambda x:(x>0).astype(float)*x)\n",
    "weights = np.array([[1,4],[4,1]])\n",
    "activation = sigmoid(np.array([1,0.01]))\n",
    "\n",
    "print(\"Sigmoid Activations\")\n",
    "activations = list()\n",
    "for iter in range(10):\n",
    "    activation = sigmoid(activation.dot(weights))\n",
    "    activations.append(activation)\n",
    "    print(activation)\n",
    "print(\"\\nSigmoid Gradient\")\n",
    "gradient = np.ones_like(activation)\n",
    "for activation in reversed(activations):\n",
    "    gradient = (activation * (1 - activation) * gradient)\n",
    "    gradient = gradient.dot(weights.transpose())\n",
    "    print(gradient)\n",
    "\n",
    "print(\"Activations\")\n",
    "activations = list()\n",
    "for iter in range(10):\n",
    "    activation = relu(activation.dot(weights))\n",
    "    activations.append(activation)\n",
    "    print(activation)\n",
    "print(\"\\nGradients\")\n",
    "gradient = np.ones_like(activation)\n",
    "for activation in reversed(activations):\n",
    "    gradient = ((activation > 0) * gradient).dot(weights.transpose())\n",
    "    print(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, inp, hidden):\n",
    "    prev_hidden, prev_cell = (hidden[0], hidden[1])\n",
    "    \n",
    "    f = (self.xf.forward(inp) + self.hf.forward(prev_hidden)).sigmoid() # 'forget' gate\n",
    "    i = (self.xi.forward(inp) + self.hi.forward(prev_hidden)).sigmoid() # input gate\n",
    "    o = (self.xo.forward(inp) + self.ho.forward(prev_hidden)).sigmoid() # output gate\n",
    "    u = (self.xc.forward(inp) + self.hc.forward(prev_hidden)).tanh()    # update gate\n",
    "    cell = (f * prev_cell) + (i * u)\n",
    "    h = o * cell.tanh()\n",
    "    output = self.w_ho.forward(h)\n",
    "    return output, (h, cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(Layer):\n",
    "    def __init__(self, n_inputs, n_hidden, n_output):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        \n",
    "        self.xf = Linear(n_inputs, n_hidden)\n",
    "        self.xi = Linear(n_inputs, n_hidden)\n",
    "        self.xo = Linear(n_inputs, n_hidden)\n",
    "        self.xc = Linear(n_inputs, n_hidden)\n",
    "        self.hf = Linear(n_inputs, n_hidden, bias=False)\n",
    "        self.hi = Linear(n_inputs, n_hidden, bias=False)\n",
    "        self.ho = Linear(n_inputs, n_hidden, bias=False)\n",
    "        self.hc = Linear(n_inputs, n_hidden, bias=False)\n",
    "        \n",
    "        self.w_ho = Linear(n_hidden, n_output, bias=False)\n",
    "        \n",
    "        self.parameters += self.xf.get_parameters()\n",
    "        self.parameters += self.xi.get_parameters()\n",
    "        self.parameters += self.xo.get_parameters()\n",
    "        self.parameters += self.xc.get_parameters()\n",
    "        self.parameters += self.hf.get_parameters()\n",
    "        self.parameters += self.hi.get_parameters()\n",
    "        self.parameters += self.ho.get_parameters()\n",
    "        self.parameters += self.hc.get_parameters()\n",
    "        \n",
    "        self.parameters += self.w_ho.get_parameters()\n",
    "        \n",
    "    def forward(self, inp, hidden):\n",
    "        prev_hidden = hidden[0]\n",
    "        prev_cell = hidden[1]\n",
    "    \n",
    "        f = (self.xf.forward(inp) + self.hf.forward(prev_hidden)).sigmoid() # 'forget' gate\n",
    "        i = (self.xi.forward(inp) + self.hi.forward(prev_hidden)).sigmoid() # input gate\n",
    "        o = (self.xo.forward(inp) + self.ho.forward(prev_hidden)).sigmoid() # output gate\n",
    "        g = (self.xc.forward(inp) + self.hc.forward(prev_hidden)).tanh()\n",
    "        c = (f * prev_cell) + (i * g)\n",
    "        h = o * c.tanh()\n",
    "        \n",
    "        output = self.w_ho.forward(h)\n",
    "        return output, (h, c)\n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        h = Tensor(np.zeros((batch_size, self.n_hidden)), autograd=True)\n",
    "        c = Tensor(np.zeros((batch_size, self.n_hidden)), autograd=True)\n",
    "        h.data[:,0] += 1\n",
    "        c.data[:,0] += 1\n",
    "        return (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, random, math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "f = open('shakespeare.txt', 'r')\n",
    "raw = f.read()\n",
    "f.close()\n",
    "\n",
    "vocab = list(set(raw))\n",
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "indices = np.array(list(map(lambda x:word2index[x], raw)))\n",
    "\n",
    "embed = Embedding(vocab_size=len(vocab), dim=512)\n",
    "model = LSTMCell(n_inputs=512, n_hidden=512, n_output=len(vocab))\n",
    "model.w_ho.weight.data *= 0\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "optim = SGD(parameters=model.get_parameters() + embed.get_parameters(), alpha=0.05)\n",
    "\n",
    "batch_size = 16\n",
    "bptt = 25\n",
    "n_batches = int((indices.shape[0] / (batch_size)))\n",
    "\n",
    "trimmed_indices = indices[:n_batches*batch_size]\n",
    "batched_indices = trimmed_indices.reshape(batch_size, n_batches)\n",
    "batched_indices = batched_indices.transpose()\n",
    "\n",
    "input_batched_indices = batched_indices[0:-1]\n",
    "target_batched_indices = batched_indices[1:]\n",
    "\n",
    "n_bptt = int(((n_batches - 1) / bptt))\n",
    "input_batches = input_batched_indices[:n_bptt*bptt]\n",
    "input_batches = input_batches.reshape(n_bptt, bptt, batch_size)\n",
    "target_batches = target_batched_indices[:n_bptt*bptt]\n",
    "target_batches = target_batches.reshape(n_bptt, bptt, batch_size)\n",
    "min_loss = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(iterations=100):\n",
    "    for iter in range(iterations):\n",
    "        total_loss, n_loss = (0,0)\n",
    "\n",
    "        hidden = model.init_hidden(batch_size=batch_size)\n",
    "        batches_to_train = len(input_batches)\n",
    "\n",
    "        for batch_i in range(batches_to_train):\n",
    "\n",
    "            hidden = (Tensor(hidden[0].data, autograd=True), Tensor(hidden[1].data, autograd=True))\n",
    "            losses = list()\n",
    "\n",
    "            for t in range(bptt):\n",
    "                inp = Tensor(input_batches[batch_i][t], autograd=True)\n",
    "                rnn_input = embed.forward(inp=inp)\n",
    "                output, hidden = model.forward(inp=rnn_input, hidden=hidden)\n",
    "\n",
    "                target = Tensor(target_batches[batch_i][t], autograd=True)\n",
    "                batch_loss = criterion.forward(output, target)\n",
    "\n",
    "                if (t == 0):\n",
    "                    losses.append(batch_loss)\n",
    "                else:\n",
    "                    losses.append(batch_loss + losses[-1])    \n",
    "            loss = losses[-1]\n",
    "\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            total_loss += loss.data / bptt\n",
    "            epoch_loss = np.exp(total_loss / (batch_i + 1))\n",
    "            if (epoch_loss < min_loss):\n",
    "                min_loss = epoch_loss\n",
    "                print()\n",
    "            log = \"\\r Iter: \" + str(iter)\n",
    "            log += \" - Alpha: \" + str(optim.alpha)[0:5]\n",
    "            log += \" - Batch \" + str(batch_i+1) + \"/\" + str(len(input_batches))\n",
    "            log += \" - Min Loss: \" + str(min_loss)[0:5]\n",
    "            log += \" - Loss: \" + str(epoch_loss)\n",
    "            if (batch_i == 0):\n",
    "                s = generate_sample(n=70, init_char=\"T\").replace(\"\\n\", \" \")\n",
    "                log += \" - \" + s\n",
    "            if (batch_i % 1 == 0):\n",
    "                sys.stdout.write(log)\n",
    "        optim.alpha *= 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'min_loss' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-84557d09d340>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-80-d8f981d99a7f>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(iterations)\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbptt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_loss\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_i\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepoch_loss\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmin_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m                 \u001b[0mmin_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepoch_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'min_loss' referenced before assignment"
     ]
    }
   ],
   "source": [
    "train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(n=30, init_char=' '):\n",
    "    s = \"\"\n",
    "    hidden = model.init_hidden(batch_size=1)\n",
    "    inp = Tensor(np.array([word2index[init_char]]))\n",
    "    for i in range(n):\n",
    "        rnn_input = embed.forward(inp)\n",
    "        output, hidden = model.forward(inp=rnn_input, hidden=hidden)\n",
    "        output.data *= 15\n",
    "        temp_dist = output.softmax()\n",
    "        temp_dist /= temp_dist.sum()\n",
    "        \n",
    "        m = output.data.argmax()\n",
    "        c = vocab[m]\n",
    "        inp = Tensor(np.array([m]))\n",
    "        s += c\n",
    "    return s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
